{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7043, 35)\n",
      "\n",
      "Feature distribution:\n",
      "         Count      Zip Code     Latitude    Longitude  Tenure Months  \\\n",
      "count  7043.0   7043.000000  7043.000000  7043.000000    7043.000000   \n",
      "mean      1.0  93521.964646    36.282441  -119.798880      32.371149   \n",
      "std       0.0   1865.794555     2.455723     2.157889      24.559481   \n",
      "min       1.0  90001.000000    32.555828  -124.301372       0.000000   \n",
      "25%       1.0  92102.000000    34.030915  -121.815412       9.000000   \n",
      "50%       1.0  93552.000000    36.391777  -119.730885      29.000000   \n",
      "75%       1.0  95351.000000    38.224869  -118.043237      55.000000   \n",
      "max       1.0  96161.000000    41.962127  -114.192901      72.000000   \n",
      "\n",
      "       Monthly Charges  Churn Value  Churn Score         CLTV  \n",
      "count      7043.000000  7043.000000  7043.000000  7043.000000  \n",
      "mean         64.761692     0.265370    58.699418  4400.295755  \n",
      "std          30.090047     0.441561    21.525131  1183.057152  \n",
      "min          18.250000     0.000000     5.000000  2003.000000  \n",
      "25%          35.500000     0.000000    40.000000  3469.000000  \n",
      "50%          70.350000     0.000000    61.000000  4527.000000  \n",
      "75%          89.850000     1.000000    75.000000  5380.500000  \n",
      "max         118.750000     1.000000   100.000000  6500.000000  \n",
      "\n",
      "Missing values:\n",
      " CustomerID              0\n",
      "Count                   0\n",
      "Country                 0\n",
      "State                   0\n",
      "City                    0\n",
      "Zip Code                0\n",
      "Lat Long                0\n",
      "Latitude                0\n",
      "Longitude               0\n",
      "Gender                  0\n",
      "Senior Citizen          0\n",
      "Partner                 0\n",
      "Dependents              0\n",
      "Tenure Months           0\n",
      "Phone Service           0\n",
      "Multiple Lines          0\n",
      "Internet Service        0\n",
      "Online Security         0\n",
      "Online Backup           0\n",
      "Device Protection       0\n",
      "Tech Support            0\n",
      "Streaming TV            0\n",
      "Streaming Movies        0\n",
      "Contract                0\n",
      "Paperless Billing       0\n",
      "Payment Method          0\n",
      "Monthly Charges         0\n",
      "Total Charges           0\n",
      "Churn Label             0\n",
      "Churn Value             0\n",
      "Churn Score             0\n",
      "CLTV                    0\n",
      "Churn Reason         5174\n",
      "conversation            0\n",
      "customer_text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/Telco_customer_churn_with_text.csv')\n",
    "\n",
    "# Initial exploration\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFeature distribution:\\n\", df.describe())\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn distribution:\n",
      "Churn Label\n",
      "No     0.73463\n",
      "Yes    0.26537\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Churn distribution:\")\n",
    "print(df['Churn Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully loaded dataset with shape: (7043, 35)\n",
      "INFO:__main__:Data validation completed\n",
      "INFO:__main__:Data preparation completed successfully\n"
     ]
    }
   ],
   "source": [
    "# data_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataPreparation:\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.power_transformer = PowerTransformer()\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        \n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"Load and validate the dataset.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            logger.info(f\"Successfully loaded dataset with shape: {df.shape}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def validate_data(self, df):\n",
    "        \"\"\"Perform basic data validation checks.\"\"\"\n",
    "        validation_report = {\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'duplicates': df.duplicated().sum(),\n",
    "            'data_types': df.dtypes.to_dict()\n",
    "        }\n",
    "        \n",
    "        # Check for invalid values in important columns\n",
    "        if 'Monthly Charges' in df.columns:\n",
    "            validation_report['negative_charges'] = (df['Monthly Charges'] < 0).sum()\n",
    "            \n",
    "        if 'Tenure Months' in df.columns:\n",
    "            validation_report['invalid_tenure'] = (df['Tenure Months'] < 0).sum()\n",
    "            \n",
    "        logger.info(\"Data validation completed\")\n",
    "        return validation_report\n",
    "    \n",
    "    def engineer_basic_features(self, df):\n",
    "        \"\"\"Create basic derived features.\"\"\"\n",
    "        df = df.copy()\n",
    "        # drop unecessary columns\n",
    "        df = df.drop(['CustomerID',\t'Count', 'Country',\t'State', 'City', 'Zip Code', 'Lat Long', 'Latitude', \n",
    "                      'Longitude', 'Churn Value', 'Churn Score',\t'CLTV',\t'Churn Reason', 'conversation', 'customer_text'], axis=1)\n",
    "\n",
    "        \n",
    "        # Customer value features\n",
    "        df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce')\n",
    "        df['Revenue_per_Month'] = df['Total Charges'] / df['Tenure Months']\n",
    "        df['Average_Monthly_Charges'] = df['Total Charges'] / df['Tenure Months']\n",
    "        df['Charges_Evolution'] = df['Monthly Charges'] - df['Average_Monthly_Charges']\n",
    "        \n",
    "        # Service usage features\n",
    "        service_columns = ['Phone Service', 'Internet Service', 'Online Security',\n",
    "                          'Online Backup', 'Device Protection', 'Tech Support',\n",
    "                          'Streaming TV', 'Streaming Movies']\n",
    "        \n",
    "        df['Total_Services'] = df[service_columns].apply(\n",
    "            lambda x: x.str.count('Yes').sum() if x.dtype == 'object' else x.sum(), axis=1\n",
    "        )\n",
    "        \n",
    "        # Customer segments\n",
    "        df['Value_Segment'] = pd.qcut(df['Monthly Charges'], q=4, \n",
    "                                    labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "        # ... your training code ...\n",
    "        bins = pd.qcut(df['Monthly Charges'], q=4, retbins=True)[1] # Get the bin edges\n",
    "        joblib.dump(bins, 'quantile_bins.pkl')  # Save the bins\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_advanced_features(self, df):\n",
    "        \"\"\"Create more sophisticated features.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Contract risk score\n",
    "        contract_risk = {'Month-to-month': 3, 'One year': 2, 'Two year': 1}\n",
    "        df['Contract_Risk_Score'] = df['Contract'].map(contract_risk)\n",
    "        \n",
    "        # Payment reliability\n",
    "        payment_risk = {\n",
    "            'Electronic check': 3,\n",
    "            'Mailed check': 2,\n",
    "            'Bank transfer (automatic)': 1,\n",
    "            'Credit card (automatic)': 1\n",
    "        }\n",
    "        df['Payment_Risk_Score'] = df['Payment Method'].map(payment_risk)\n",
    "        \n",
    "        # Service dependency score\n",
    "        service_weights = {\n",
    "            'Phone Service': 1,\n",
    "            'Internet Service': 2,\n",
    "            'Online Security': 0.5,\n",
    "            'Online Backup': 0.5,\n",
    "            'Device Protection': 0.5,\n",
    "            'Tech Support': 0.5,\n",
    "            'Streaming TV': 1,\n",
    "            'Streaming Movies': 1\n",
    "        }\n",
    "        \n",
    "        df['Service_Dependency_Score'] = sum(\n",
    "            (df[service] == 'Yes').astype(int) * weight\n",
    "            for service, weight in service_weights.items()\n",
    "        )\n",
    "        \n",
    "        # Loyalty-adjusted value\n",
    "        df['Loyalty_Adjusted_Value'] = (\n",
    "            df['Monthly Charges'] * np.log1p(df['Tenure Months'])\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def encode_categorical_features(self, df):\n",
    "        \"\"\"Encode categorical variables with proper handling.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Features for label encoding\n",
    "        label_encode_cols = ['Gender', 'Contract', 'Payment Method']\n",
    "        \n",
    "        # Features for one-hot encoding\n",
    "        onehot_cols = ['Internet Service', 'Value_Segment']\n",
    "        \n",
    "        # Label encoding\n",
    "        for col in label_encode_cols:\n",
    "            if col in df.columns:\n",
    "                self.label_encoders[col] = LabelEncoder()\n",
    "                df[f'{col}_Encoded'] = self.label_encoders[col].fit_transform(df[col])\n",
    "        \n",
    "        # One-hot encoding\n",
    "        df = pd.get_dummies(df, columns=onehot_cols, prefix=onehot_cols)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scale_numerical_features(self, df):\n",
    "        \"\"\"Scale numerical features with proper handling of skewness.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Basic numerical features\n",
    "        basic_num_cols = ['Monthly Charges', 'Total Charges', 'Tenure Months']\n",
    "        \n",
    "        # Derived numerical features\n",
    "        derived_num_cols = ['Revenue_per_Month', 'Average_Monthly_Charges',\n",
    "                          'Charges_Evolution', 'Service_Dependency_Score',\n",
    "                          'Loyalty_Adjusted_Value']\n",
    "        \n",
    "        all_num_cols = [col for col in basic_num_cols + derived_num_cols \n",
    "                       if col in df.columns]\n",
    "        \n",
    "        # Handle missing values\n",
    "        df[all_num_cols] = self.imputer.fit_transform(df[all_num_cols])\n",
    "        \n",
    "        # Apply power transform for heavily skewed features\n",
    "        df[all_num_cols] = self.power_transformer.fit_transform(df[all_num_cols])\n",
    "        \n",
    "        # Standard scaling\n",
    "        df[all_num_cols] = self.scaler.fit_transform(df[all_num_cols])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self, file_path):\n",
    "        \"\"\"Complete data preparation pipeline.\"\"\"\n",
    "        # Load and validate\n",
    "        df = self.load_data(file_path)\n",
    "        validation_report = self.validate_data(df)\n",
    "        \n",
    "        if validation_report['duplicates'] > 0:\n",
    "            logger.warning(f\"Found {validation_report['duplicates']} duplicate rows\")\n",
    "            df = df.drop_duplicates()\n",
    "        \n",
    "        # Feature engineering\n",
    "        df = self.engineer_basic_features(df)\n",
    "        df = self.engineer_advanced_features(df)\n",
    "        \n",
    "        # Encoding and scaling\n",
    "        df = self.encode_categorical_features(df)\n",
    "        df = self.scale_numerical_features(df)\n",
    "        \n",
    "        logger.info(\"Data preparation completed successfully\")\n",
    "        return df, validation_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prep = DataPreparation()\n",
    "    processed_df, validation_report = prep.prepare_data('./data/Telco_customer_churn_with_text.csv')\n",
    "    processed_df.to_csv('./data/processed_telco_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mutual_info_classif\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# feature_analysis.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import math\n",
    "import os\n",
    "\n",
    "class FeatureAnalysis:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        self.categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "    def plot_feature_distributions(self, save_path=None):\n",
    "        \"\"\"Plot distributions of numerical features.\"\"\"\n",
    "        num_features = len(self.numerical_cols)\n",
    "        num_cols = 4  # Set columns to 4 for better spacing\n",
    "        num_rows = math.ceil(num_features / num_cols)  # Adjust rows dynamically\n",
    "        \n",
    "        plt.figure(figsize=(15, 5 * num_rows))  # Adjust figure size based on rows\n",
    "        for i, col in enumerate(self.numerical_cols, 1):\n",
    "            plt.subplot(num_rows, num_cols, i)\n",
    "            sns.histplot(data=self.df, x=col, hue='Churn Label', alpha=0.5)\n",
    "            plt.title(f'{col} Distribution')\n",
    "            plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(save_path, 'feature_distributions.png'))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_correlation_matrix(self, save_path=None):\n",
    "        \"\"\"Plot correlation matrix of numerical features.\"\"\"\n",
    "        corr_matrix = self.df[self.numerical_cols].corr()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(save_path, 'correlation_matrix.png'))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_feature_importance(self, target_col='Churn Label', save_path=None):\n",
    "        \"\"\"Calculate and plot feature importance using mutual information.\"\"\"\n",
    "        X = self.df[self.numerical_cols]\n",
    "        y = (self.df[target_col] == 'Yes').astype(int)\n",
    "        \n",
    "        # Calculate mutual information scores\n",
    "        mi_scores = mutual_info_classif(X, y)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': self.numerical_cols,\n",
    "            'Importance': mi_scores\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "        plt.title('Feature Importance (Mutual Information)')\n",
    "\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(save_path, 'feature_importance.png'))\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def plot_categorical_analysis(self, save_path=None):\n",
    "        \"\"\"Analyze categorical features relationship with churn.\"\"\"\n",
    "        num_features = len(self.categorical_cols)\n",
    "        num_cols = 3  # Set to 3 columns for a cleaner view\n",
    "        num_rows = math.ceil(num_features / num_cols)  # Dynamically adjust rows\n",
    "\n",
    "        plt.figure(figsize=(15, 5 * num_rows))\n",
    "        for i, col in enumerate(self.categorical_cols, 1):\n",
    "            if col != 'Churn Label':\n",
    "                plt.subplot(num_rows, num_cols, i)\n",
    "                churn_props = self.df.groupby(col)['Churn Label'].value_counts(normalize=True).unstack()\n",
    "                if 'Yes' in churn_props.columns:\n",
    "                    churn_props['Yes'].sort_values().plot(kind='bar')\n",
    "                else:\n",
    "                    churn_props.plot(kind='bar', stacked=True)\n",
    "                plt.title(f'Churn Rate by {col}')\n",
    "                plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(save_path, 'categorical_analysis.png'))\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_feature_report(self, save_path=None):\n",
    "        \"\"\"Generate a comprehensive feature analysis report.\"\"\"\n",
    "        report = {\n",
    "            'numerical_stats': self.df[self.numerical_cols].describe(),\n",
    "            'categorical_stats': {\n",
    "                col: self.df[col].value_counts(normalize=True)\n",
    "                for col in self.categorical_cols\n",
    "            },\n",
    "            'missing_values': self.df.isnull().sum(),\n",
    "            'correlation_analysis': self.df[self.numerical_cols].corr()\n",
    "        }\n",
    "        \n",
    "        if save_path:\n",
    "            with open(f'{save_path}/feature_report.txt', 'w') as f:\n",
    "                for section, data in report.items():\n",
    "                    f.write(f'\\n{section.upper()}\\n{\"=\"*50}\\n')\n",
    "                    f.write(str(data))\n",
    "                    f.write('\\n\\n')\n",
    "                    \n",
    "        return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load processed data\n",
    "    df = pd.read_csv('./data/processed_telco_data.csv')\n",
    "    #df = df.drop(columns=['conversation', 'customer_text'])\n",
    "    \n",
    "    # Create analysis object\n",
    "    analyzer = FeatureAnalysis(df)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    analyzer.plot_feature_distributions('outputs')\n",
    "    analyzer.plot_correlation_matrix('outputs')\n",
    "    analyzer.plot_feature_importance(target_col='Churn Label', save_path='outputs')\n",
    "    analyzer.plot_categorical_analysis('outputs')\n",
    "    \n",
    "    # Generate report\n",
    "    report = analyzer.generate_feature_report('outputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boruta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboruta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BorutaPy\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     10\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'boruta'"
     ]
    }
   ],
   "source": [
    "# feature_selection.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from boruta import BorutaPy\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FeatureSelector:\n",
    "    def __init__(self, df, target_col='Churn Label'):\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.le = LabelEncoder()\n",
    "        \n",
    "        # Ensure only numerical columns are used\n",
    "        self.numerical_cols = self.df.select_dtypes(include=['number']).columns.tolist()\n",
    "        if self.target_col in self.numerical_cols:\n",
    "            self.numerical_cols.remove(self.target_col)\n",
    "    \n",
    "    def prepare_target(self):\n",
    "        \"\"\"Prepare target variable for selection methods.\"\"\"\n",
    "        return self.le.fit_transform(self.df[self.target_col])\n",
    "    \n",
    "    def filter_method(self, k=10):\n",
    "        \"\"\"Select features using mutual information.\"\"\"\n",
    "        y = self.prepare_target()\n",
    "        X = self.df[self.numerical_cols]\n",
    "        \n",
    "        # Select features\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        selected_features = X.columns[selector.get_support()].tolist()\n",
    "        feature_scores = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Score': selector.scores_\n",
    "        }).sort_values('Score', ascending=False)\n",
    "        \n",
    "        logger.info(f\"Selected {len(selected_features)} features using mutual information\")\n",
    "        return selected_features, feature_scores\n",
    "    \n",
    "    def wrapper_method(self, n_features=10):\n",
    "        \"\"\"Select features using Recursive Feature Elimination.\"\"\"\n",
    "        y = self.prepare_target()\n",
    "        X = self.df[self.numerical_cols]\n",
    "        \n",
    "        # Initialize estimator\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        selector = RFE(estimator=estimator, n_features_to_select=n_features)\n",
    "        \n",
    "        # Fit selector\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        selected_features = X.columns[selector.support_].tolist()\n",
    "        feature_ranks = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Rank': selector.ranking_\n",
    "        }).sort_values('Rank')\n",
    "        \n",
    "        logger.info(f\"Selected {len(selected_features)} features using RFE\")\n",
    "        return selected_features, feature_ranks\n",
    "    \n",
    "    def boruta_selection(self):\n",
    "        \"\"\"Select features using Boruta algorithm.\"\"\"\n",
    "        y = self.prepare_target()\n",
    "        X = self.df[self.numerical_cols]\n",
    "        \n",
    "        # Initialize Random Forest classifier\n",
    "        rf = RandomForestClassifier(n_jobs=-1, max_depth=5, random_state=42)\n",
    "        \n",
    "        # Initialize Boruta\n",
    "        boruta = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42)\n",
    "        \n",
    "        # Fit Boruta\n",
    "        boruta.fit(X.values, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        selected_features = X.columns[boruta.support_].tolist()\n",
    "        feature_ranks = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Boruta_Ranking': boruta.ranking_\n",
    "        }).sort_values('Boruta_Ranking')\n",
    "        \n",
    "        logger.info(f\"Selected {len(selected_features)} features using Boruta\")\n",
    "        return selected_features, feature_ranks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('./data/processed_telco_data.csv')\n",
    "    selector = FeatureSelector(df)\n",
    "    \n",
    "    selected_kbest, kbest_scores = selector.filter_method(k=10)\n",
    "    selected_rfe, rfe_ranks = selector.wrapper_method(n_features=10)\n",
    "    selected_boruta, boruta_ranks = selector.boruta_selection()\n",
    "    # store features\n",
    "    joblib.dump(selected_boruta, \"baseline_boruta_features.pkl\")\n",
    "    \n",
    "    print(\"Top Features from SelectKBest:\", selected_kbest)\n",
    "    print(\"Top Features from RFE:\", selected_rfe)\n",
    "    print(\"Top Features from Boruta:\", selected_boruta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feature_selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureSelector\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Setup logging\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'feature_selection'"
     ]
    }
   ],
   "source": [
    "# baseline_model.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import logging\n",
    "from feature_selection import FeatureSelector\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(\"./data/processed_telco_data.csv\")\n",
    "\n",
    "# Feature Selection\n",
    "selector = FeatureSelector(df)\n",
    "selected_features =  joblib.load(\"baseline_boruta_features.pkl\")  # Use boruta\n",
    "\n",
    "# Prepare Data\n",
    "X = df[selected_features]\n",
    "y = df[\"Churn Label\"].apply(lambda x: 1 if x == \"Yes\" else 0)  # Convert target to binary\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save Scaler\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Define Model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best Model\n",
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, \"baseline_churn_model.pkl\")\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "logger.info(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "logger.info(f\"Classification Report:\\n {classification_report(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feature_selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureSelector \u001b[38;5;66;03m# Make sure this is importable\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_churn\u001b[39m(data, model_path, scaler_path):\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predicts churn from a JSON-like data structure.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'feature_selection'"
     ]
    }
   ],
   "source": [
    "# prediction_pipeline.py\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from feature_selection import FeatureSelector # Make sure this is importable\n",
    "\n",
    "def predict_churn(data, model_path, scaler_path):\n",
    "    \"\"\"Predicts churn from a JSON-like data structure.\"\"\"\n",
    "    try:\n",
    "        # 1. Convert JSON-like data to DataFrame\n",
    "        try:  # Handle cases where data might already be a dataframe\n",
    "            new_df = pd.DataFrame(data)\n",
    "        except TypeError: # Data is probably a list of dictionaries.\n",
    "            new_df = pd.DataFrame([data]) if isinstance(data, dict) else pd.DataFrame(data)\n",
    "        \n",
    "        # preprocess data\n",
    "        prep = DataPreparation()\n",
    "        new_df, validation_report = prep.prepare_data(new_df)\n",
    "\n",
    "        # 2. Feature Selection (Crucial for consistency)\n",
    "        selector = FeatureSelector(new_df)\n",
    "        selected_features, _ = selector.boruta_selection()\n",
    "        X_new = new_df[selected_features]\n",
    "\n",
    "        # 3. Handle Missing Columns (Robustness)\n",
    "        missing_cols = set(X_new.columns) - set(selected_features)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns in input data: {missing_cols}\")\n",
    "\n",
    "        # 4. Scale Data\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "        # 5. Load Model\n",
    "        model = joblib.load(model_path)\n",
    "\n",
    "        # 6. Make Predictions\n",
    "        y_prob = model.predict_proba(X_new_scaled)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)  # Or adjust threshold\n",
    "\n",
    "        # 7. Create Response (Dictionary for JSON)\n",
    "        predictions = {\n",
    "            \"churn_probability\": y_prob.tolist(),  # Convert to list for JSON\n",
    "            \"churn_prediction\": y_pred.tolist()  # Convert to list for JSON\n",
    "        }\n",
    "\n",
    "        # Include original data if needed for debugging/analysis\n",
    "        # predictions[\"input_data\"] = data  # Convert to list for JSON\n",
    "        return predictions\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": \"Model or scaler file not found.\"}\n",
    "    except ValueError as e:\n",
    "        return {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# FastAPI app (main.py)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, HTTPException, Request\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Union\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "# FastAPI app (main.py)\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Union\n",
    "# Import predict_churn \n",
    "from prediction_pipeline import predict_churn\n",
    "app = FastAPI()\n",
    "\n",
    "# Define input data model (Pydantic) for validation\n",
    "class CustomerData(BaseModel):\n",
    "    # Define your expected features here:\n",
    "\n",
    "    Gender: str = None  # Make all fields optional, or provide default values\n",
    "    SeniorCitizen: str = Field(None, alias=\"Senior Citizen\")\n",
    "    Partner: str = None\n",
    "    Dependents: str = None\n",
    "    TenureMonths: float = Field(None, alias=\"Tenure Months\")\n",
    "    PhoneService: str = Field(None, alias=\"Phone Service\")\n",
    "    MultipleLines: str = Field(None, alias=\"Multiple Lines\")\n",
    "    InternetService: str = Field(None, alias=\"Internet Service\")\n",
    "    OnlineSecurity: str = Field(None, alias=\"Online Security\")\n",
    "    OnlineBackup: str = Field(None, alias=\"Online Backup\")\n",
    "    DeviceProtection: str = Field(None, alias=\"Device Protection\")\n",
    "    TechSupport: str = Field(None, alias=\"Tech Support\")\n",
    "    StreamingTV: str = Field(None, alias=\"Streaming TV\")\n",
    "    StreamingMovies: str = Field(None, alias=\"Streaming Movies\")\n",
    "    Contract: str = None\n",
    "    PaperlessBilling: str = Field(None, alias=\"Paperless Billing\")\n",
    "    PaymentMethod: str = Field(None, alias=\"Payment Method\")\n",
    "    MonthlyCharges: float = Field(None, alias=\"Monthly Charges\")\n",
    "    TotalCharges: str = Field(None, alias=\"Total Charges\")\n",
    "class Config:\n",
    "        allow_population_by_field_name = True\n",
    "        \n",
    "@app.post(\"/predict\")\n",
    "async def predict_endpoint(request: Request, data: Union[CustomerData, List[CustomerData], Dict, List[Dict]]): \n",
    "    print(data)  # Print the data\n",
    "    import logging\n",
    "    logging.info(f\"Received data: {data}\") # Log the data\n",
    "    # Accept single or batch\n",
    "    \"\"\"Endpoint for making churn predictions.\"\"\"\n",
    "\n",
    "    model_path = \"../models/baseline_churn_model.pkl\"\n",
    "    scaler_path = \"../models/scaler.pkl\"\n",
    "\n",
    "    try:\n",
    "        if isinstance(data, CustomerData):  # Single prediction\n",
    "            data = data.dict()\n",
    "        elif isinstance(data, list) and all(isinstance(item, CustomerData) for item in data): # Batch prediction of validated data\n",
    "            data = [item.dict() for item in data]\n",
    "        elif isinstance(data, dict): # Single prediction with a dictionary\n",
    "            data = data\n",
    "        elif isinstance(data, list) and all(isinstance(item, dict) for item in data): # Batch prediction with list of dictionaries\n",
    "            data = data\n",
    "        else:\n",
    "            raise ValueError(\"Invalid data format. Please provide a dictionary or a list of dictionaries or Pydantic model instances.\")\n",
    "\n",
    "        predictions = predict_churn(data, model_path, scaler_path)\n",
    "        return predictions\n",
    "\n",
    "    except ValueError as ve:\n",
    "        raise HTTPException(status_code=400, detail=str(ve))  # Bad Request\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"An error occurred: {str(e)}\")  # Internal Server Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to acquire lock 2056144632528 on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\3ed34255a7cb8e6706a8bb21993836e99e7b959f.lock\n",
      "DEBUG:filelock:Lock 2056144632528 acquired on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\3ed34255a7cb8e6706a8bb21993836e99e7b959f.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1367cd3b9d4f5fa457624ffb4fc736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro5\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "DEBUG:filelock:Attempting to release lock 2056144632528 on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\3ed34255a7cb8e6706a8bb21993836e99e7b959f.lock\n",
      "DEBUG:filelock:Lock 2056144632528 released on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\3ed34255a7cb8e6706a8bb21993836e99e7b959f.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 2056164699344 on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\b57fe5dfcb8ec3f9bab35ed427c3434e3c7dd1ba.lock\n",
      "DEBUG:filelock:Lock 2056164699344 acquired on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\b57fe5dfcb8ec3f9bab35ed427c3434e3c7dd1ba.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe9820dbeb94602b4eed8cd240c8d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to release lock 2056164699344 on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\b57fe5dfcb8ec3f9bab35ed427c3434e3c7dd1ba.lock\n",
      "DEBUG:filelock:Lock 2056164699344 released on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\b57fe5dfcb8ec3f9bab35ed427c3434e3c7dd1ba.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 2056173369488 on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock\n",
      "DEBUG:filelock:Lock 2056173369488 acquired on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd78cb8c2d04b7a9b410c5d661c22bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to release lock 2056173369488 on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock\n",
      "DEBUG:filelock:Lock 2056173369488 released on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 2056217435536 on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\7c3919835e442510166d267fe7cbe847e0c51cd26d9ba07b89a57b952b49b8aa.lock\n",
      "DEBUG:filelock:Lock 2056217435536 acquired on C:\\Users\\Nitro5\\.cache\\huggingface\\hub\\.locks\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\\7c3919835e442510166d267fe7cbe847e0c51cd26d9ba07b89a57b952b49b8aa.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1eb1caf0705405a9b5e1abb69a478ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentiment_extractor.py\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"./data/Telco_customer_churn_with_text.csv\")  # Replace \"your_file.csv\" with the actual file name\n",
    "\n",
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    return model.config.id2label[predicted_class_id]\n",
    "\n",
    "# Apply the sentiment analysis function to the 'customer_text' column\n",
    "df['customer_sentiment'] = df['customer_text'].apply(get_sentiment)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file (optional)\n",
    "df.to_csv(\"./data/Telco_customer_churn_with_sentiment.csv\", index=False)  # Replace with your desired file name\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_features_extractor\n",
    "\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "# load_dotenv()\n",
    "# hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name: str = \"bert-base-uncased\" ): #api_key: str = hf_token\n",
    "        self.feature_extractor = pipeline(\n",
    "            \"feature-extraction\",\n",
    "            framework=\"pt\",\n",
    "            model=model_name,\n",
    "            # api_key=api_key\n",
    "        )\n",
    "        \n",
    "    def get_features(self, texts: Union[str, List[str]], return_df: bool = True) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract features from one or more texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: Single text string or list of texts\n",
    "            return_df: If True, returns pandas DataFrame; if False, returns numpy array\n",
    "            \n",
    "        Returns:\n",
    "            Features as either numpy array or pandas DataFrame\n",
    "        \"\"\"\n",
    "        # Convert single text to list\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        # Extract features for all texts\n",
    "        all_features = []\n",
    "        for text in texts:\n",
    "            # Extract features using your working approach\n",
    "            features = self.feature_extractor(text, return_tensors=\"pt\")[0]\n",
    "            # Convert to numpy and take mean across tokens\n",
    "            reduced_features = features.numpy().mean(axis=0)\n",
    "            all_features.append(reduced_features)\n",
    "            \n",
    "        # Stack all features into a single array\n",
    "        feature_array = np.stack(all_features)\n",
    "            \n",
    "        if return_df:\n",
    "            # Convert to DataFrame with feature column names\n",
    "            feature_columns = [f'feature_{i}' for i in range(feature_array.shape[1])]\n",
    "            return pd.DataFrame(feature_array, columns=feature_columns)\n",
    "            \n",
    "        return feature_array\n",
    "\n",
    "def process_csv_file(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Process a CSV file by extracting features from the customer_text column\n",
    "    and saving results to a new CSV file.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input CSV file\n",
    "        output_file: Path to output CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        print(f\"Reading input file: {input_file}\")\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Verify customer_text column exists\n",
    "        if 'customer_text' not in df.columns:\n",
    "            raise ValueError(\"customer_text column not found in CSV file\")\n",
    "        \n",
    "        # Initialize feature extractor\n",
    "        print(\"Initializing feature extractor...\")\n",
    "        extractor = TextFeatureExtractor()\n",
    "        \n",
    "        # Extract features from customer_text column\n",
    "        print(\"Extracting features...\")\n",
    "        features_df = extractor.get_features(df['customer_text'].tolist())\n",
    "        \n",
    "        # Combine original data with features\n",
    "        print(\"Combining features with original data...\")\n",
    "        result_df = pd.concat([df, features_df], axis=1)\n",
    "        \n",
    "        # Save to new CSV file\n",
    "        print(f\"Saving results to: {output_file}\")\n",
    "        result_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"Processing completed successfully!\")\n",
    "        print(f\"Number of rows processed: {len(df)}\")\n",
    "        print(f\"Number of features extracted: {len(features_df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    input_file = \"./data/Telco_customer_churn_with_sentiment.csv\"  # Your input CSV file\n",
    "    output_file = \"./data/Telco_customer_churn_with_features.csv\"  # Output file name\n",
    "    \n",
    "    process_csv_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textual_features.py\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name: str = 'all-mpnet-base-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor with a sentence transformer model.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def get_features(self, texts: Union[str, List[str]], \n",
    "                    batch_size: int = 32,\n",
    "                    return_df: bool = True) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        # Convert single text to list\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Extract embeddings with batching\n",
    "        feature_array = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        if return_df:\n",
    "            # Convert to DataFrame with feature column names\n",
    "            feature_columns = [f'feature_{i}' for i in range(feature_array.shape[1])]\n",
    "            return pd.DataFrame(feature_array, columns=feature_columns)\n",
    "        \n",
    "        return feature_array\n",
    "\n",
    "def process_csv_file(input_file: str, output_file: str, batch_size: int = 32):\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        print(f\"Reading input file: {input_file}\")\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Verify customer_text column exists\n",
    "        if 'customer_text' not in df.columns:\n",
    "            raise ValueError(\"customer_text column not found in CSV file\")\n",
    "        \n",
    "        # Initialize feature extractor\n",
    "        print(\"Initializing feature extractor...\")\n",
    "        extractor = TextFeatureExtractor()\n",
    "        \n",
    "        # Extract features from customer_text column\n",
    "        print(\"Extracting features...\")\n",
    "        features_df = extractor.get_features(\n",
    "            df['customer_text'].tolist(),\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Combine original data with features\n",
    "        print(\"Combining features with original data...\")\n",
    "        result_df = pd.concat([df, features_df], axis=1)\n",
    "        \n",
    "        # Save to new CSV file\n",
    "        print(f\"Saving results to: {output_file}\")\n",
    "        result_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"Processing completed successfully!\")\n",
    "        print(f\"Number of rows processed: {len(df)}\")\n",
    "        print(f\"Number of features extracted: {len(features_df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"./data/Telco_customer_churn_with_sentiment.csv\"\n",
    "    output_file = \"./data/Telco_customer_churn_with_features_.csv\"\n",
    "    \n",
    "    process_csv_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "sentence = [\"My internet service is so poor these days!\"]\n",
    "embeddings = model.encode(sentence)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('./data/Telco_customer_churn_with_features.csv')\n",
    "\n",
    "# Extract embedding features\n",
    "embedding_features = df.loc[:, 'feature_0':'feature_767'].values\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)  # Retain 95% of variance\n",
    "reduced_features = pca.fit_transform(embedding_features)\n",
    "\n",
    "# Label Encode 'customer_sentiment'\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment_encoded'] = label_encoder.fit_transform(df['customer_sentiment'])\n",
    "\n",
    "# Combine reduced features and encoded sentiment\n",
    "combined_features = np.hstack((reduced_features, df[['sentiment_encoded']].values))\n",
    "\n",
    "# Create a DataFrame with column names\n",
    "# Generate names for reduced features\n",
    "reduced_feature_names = [f'pca_{i}' for i in range(reduced_features.shape[1])]\n",
    "\n",
    "# Combine all column names\n",
    "column_names = reduced_feature_names + ['sentiment_encoded']\n",
    "\n",
    "# Create the DataFrame\n",
    "processed_df = pd.DataFrame(combined_features, columns=column_names)\n",
    "\n",
    "# Display the first few rows\n",
    "print(processed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Senior Citizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Tenure Months</th>\n",
       "      <th>Phone Service</th>\n",
       "      <th>Multiple Lines</th>\n",
       "      <th>Online Security</th>\n",
       "      <th>Online Backup</th>\n",
       "      <th>Device Protection</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>pca_9</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>-1.495444</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>2.607593</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.055595</td>\n",
       "      <td>-0.023386</td>\n",
       "      <td>-0.181521</td>\n",
       "      <td>0.178755</td>\n",
       "      <td>-0.031445</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-1.495444</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>2.670311</td>\n",
       "      <td>-0.065715</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>-0.185605</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.195710</td>\n",
       "      <td>0.159138</td>\n",
       "      <td>0.102147</td>\n",
       "      <td>0.059345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.926287</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>2.564315</td>\n",
       "      <td>-0.045868</td>\n",
       "      <td>-0.006414</td>\n",
       "      <td>0.327256</td>\n",
       "      <td>-0.150967</td>\n",
       "      <td>-0.013058</td>\n",
       "      <td>-0.016928</td>\n",
       "      <td>-0.087543</td>\n",
       "      <td>0.042756</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.080538</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.869206</td>\n",
       "      <td>1.232879</td>\n",
       "      <td>0.221853</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.761599</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.869206</td>\n",
       "      <td>1.232879</td>\n",
       "      <td>0.221853</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender Senior Citizen Partner Dependents  Tenure Months Phone Service  \\\n",
       "0    Male             No      No         No      -1.495444           Yes   \n",
       "1  Female             No      No        Yes      -1.495444           Yes   \n",
       "2  Female             No      No        Yes      -0.926287           Yes   \n",
       "3  Female             No     Yes        Yes       0.080538           Yes   \n",
       "4    Male             No      No        Yes       0.761599           Yes   \n",
       "\n",
       "  Multiple Lines Online Security Online Backup Device Protection  ...  \\\n",
       "0             No             Yes           Yes                No  ...   \n",
       "1             No              No            No                No  ...   \n",
       "2            Yes              No            No               Yes  ...   \n",
       "3            Yes              No            No               Yes  ...   \n",
       "4            Yes              No           Yes               Yes  ...   \n",
       "\n",
       "      pca_1     pca_2     pca_3     pca_4     pca_5     pca_6     pca_7  \\\n",
       "0  2.607593 -0.021859 -0.055595 -0.023386 -0.181521  0.178755 -0.031445   \n",
       "1  2.670311 -0.065715 -0.013699 -0.185605  0.117329  0.195710  0.159138   \n",
       "2  2.564315 -0.045868 -0.006414  0.327256 -0.150967 -0.013058 -0.016928   \n",
       "3 -2.869206  1.232879  0.221853 -0.000614  0.002044  0.000956 -0.000875   \n",
       "4 -2.869206  1.232879  0.221853 -0.000614  0.002044  0.000956 -0.000875   \n",
       "\n",
       "      pca_8     pca_9  sentiment_encoded  \n",
       "0  0.002200  0.002916                0.0  \n",
       "1  0.102147  0.059345                0.0  \n",
       "2 -0.087543  0.042756                0.0  \n",
       "3  0.000321 -0.000104                0.0  \n",
       "4  0.000321 -0.000104                0.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "df['sentiment_encoded'] = label_encoder.fit_transform(df['customer_sentiment'])\n",
    "text_features = pd.concat([ df.loc[:, 'feature_0':'feature_767'], df['sentiment_encoded'] ], axis=1)\n",
    "pre_processed_df = pd.read_csv('./data/processed_telco_data.csv')\n",
    "concatenated_df = pd.concat([pre_processed_df, processed_df.reset_index(drop=True)], axis=1)\n",
    "concatenated_df.to_csv(\"./data/model_data.csv\")\n",
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Selected 10 features using mutual information\n",
      "INFO:__main__:Selected 10 features using RFE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t25\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t25\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t25\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t25\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t25\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t25\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t25\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t2\n",
      "Rejected: \t0\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t10 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t11 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t12 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t13 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t14 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t15 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t16 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t17 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t18 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t19 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t20 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t21 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t22 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t23 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t24 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t25 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t26 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t27 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t28 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t29 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t30 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t31 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t32 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t33 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t34 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t35 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t36 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t37 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t38 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t39 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t40 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t41 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t42 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t43 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t44 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t45 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t46 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t47 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t48 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t49 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t50 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t51 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t52 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t53 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t54 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t55 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t56 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t57 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t58 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t59 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t60 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t61 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t62 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t63 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t64 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t65 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t66 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t67 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t68 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t69 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t70 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t71 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t72 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t73 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t74 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t75 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t76 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t77 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t78 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t79 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t80 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t81 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t82 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t83 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t84 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t85 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t86 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t87 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t88 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t89 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t90 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t91 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t92 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t93 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t94 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t95 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t96 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t97 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Iteration: \t98 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Selected 23 features using Boruta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t99 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t100 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t1\n",
      "Rejected: \t1\n",
      "Top Features from SelectKBest: ['Contract_Risk_Score', 'Contract_Encoded', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7']\n",
      "Top Features from RFE: ['Tenure Months', 'Monthly Charges', 'Total Charges', 'Revenue_per_Month', 'Average_Monthly_Charges', 'Charges_Evolution', 'Contract_Risk_Score', 'Loyalty_Adjusted_Value', 'Contract_Encoded', 'pca_0']\n",
      "Top Features from Boruta: ['Tenure Months', 'Monthly Charges', 'Total Charges', 'Revenue_per_Month', 'Average_Monthly_Charges', 'Charges_Evolution', 'Total_Services', 'Contract_Risk_Score', 'Payment_Risk_Score', 'Service_Dependency_Score', 'Loyalty_Adjusted_Value', 'Contract_Encoded', 'Payment Method_Encoded', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8', 'pca_9']\n"
     ]
    }
   ],
   "source": [
    "# feature_selection.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from boruta import BorutaPy\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FeatureSelector:\n",
    "    def __init__(self, df, target_col='Churn Label'):\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.le = LabelEncoder()\n",
    "        \n",
    "        # Ensure only numerical columns are used\n",
    "        self.numerical_cols = self.df.select_dtypes(include=['number']).columns.tolist()\n",
    "        if self.target_col in self.numerical_cols:\n",
    "            self.numerical_cols.remove(self.target_col)\n",
    "    \n",
    "    def prepare_target(self):\n",
    "        \"\"\"Prepare target variable for selection methods.\"\"\"\n",
    "        return self.le.fit_transform(self.df[self.target_col])\n",
    "    \n",
    "    def filter_method(self, k=10):\n",
    "        \"\"\"Select features using mutual information.\"\"\"\n",
    "        y = self.prepare_target()\n",
    "        X = self.df[self.numerical_cols]\n",
    "        \n",
    "        # Select features\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        selected_features = X.columns[selector.get_support()].tolist()\n",
    "        feature_scores = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Score': selector.scores_\n",
    "        }).sort_values('Score', ascending=False)\n",
    "        \n",
    "        logger.info(f\"Selected {len(selected_features)} features using mutual information\")\n",
    "        return selected_features, feature_scores\n",
    "    \n",
    "    def wrapper_method(self, n_features=10):\n",
    "        \"\"\"Select features using Recursive Feature Elimination.\"\"\"\n",
    "        y = self.prepare_target()\n",
    "        X = self.df[self.numerical_cols]\n",
    "        \n",
    "        # Initialize estimator\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        selector = RFE(estimator=estimator, n_features_to_select=n_features)\n",
    "        \n",
    "        # Fit selector\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        selected_features = X.columns[selector.support_].tolist()\n",
    "        feature_ranks = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Rank': selector.ranking_\n",
    "        }).sort_values('Rank')\n",
    "        \n",
    "        logger.info(f\"Selected {len(selected_features)} features using RFE\")\n",
    "        return selected_features, feature_ranks\n",
    "    \n",
    "    def boruta_selection(self):\n",
    "        \"\"\"Select features using Boruta algorithm.\"\"\"\n",
    "        y = self.prepare_target()\n",
    "        X = self.df[self.numerical_cols]\n",
    "        \n",
    "        # Initialize Random Forest classifier\n",
    "        rf = RandomForestClassifier(n_jobs=-1, max_depth=5, random_state=42)\n",
    "        \n",
    "        # Initialize Boruta\n",
    "        boruta = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42)\n",
    "        \n",
    "        # Fit Boruta\n",
    "        boruta.fit(X.values, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        selected_features = X.columns[boruta.support_].tolist()\n",
    "        feature_ranks = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Boruta_Ranking': boruta.ranking_\n",
    "        }).sort_values('Boruta_Ranking')\n",
    "        \n",
    "        logger.info(f\"Selected {len(selected_features)} features using Boruta\")\n",
    "        return selected_features, feature_ranks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = concatenated_df\n",
    "    selector = FeatureSelector(df)\n",
    "    \n",
    "    selected_kbest, kbest_scores = selector.filter_method(k=10)\n",
    "    selected_rfe, rfe_ranks = selector.wrapper_method(n_features=10)\n",
    "    selected_boruta, boruta_ranks = selector.boruta_selection()\n",
    "\n",
    "    joblib.dump(selected_boruta, \"boruta_features.pkl\")\n",
    "    \n",
    "    print(\"Top Features from SelectKBest:\", selected_kbest)\n",
    "    print(\"Top Features from RFE:\", selected_rfe)\n",
    "    print(\"Top Features from Boruta:\", selected_boruta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Accuracy: 0.7920511000709723\n",
      "INFO:__main__:Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86      1035\n",
      "           1       0.64      0.51      0.56       374\n",
      "\n",
      "    accuracy                           0.79      1409\n",
      "   macro avg       0.73      0.70      0.71      1409\n",
      "weighted avg       0.78      0.79      0.78      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# churn_model.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import logging\n",
    "from feature_selection import FeatureSelector\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load Data\n",
    "df = concatenated_df\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "selected_features = joblib.load('boruta_features.pkl')\n",
    "\n",
    "# Prepare Data\n",
    "X = df[selected_features]\n",
    "y = df[\"Churn Label\"].apply(lambda x: 1 if x == \"Yes\" else 0)  # Convert target to binary\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save Scaler\n",
    "joblib.dump(scaler, \"churn_scaler.pkl\")\n",
    "\n",
    "# Define Model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best Model\n",
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, \"./models/churn_model.pkl\")\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "logger.info(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "logger.info(f\"Classification Report:\\n {classification_report(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI app (main.py)\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Union\n",
    "# Import predict_churn \n",
    "from prediction_pipeline import predict_churn\n",
    "app = FastAPI()\n",
    "\n",
    "# Define input data model (Pydantic) for validation\n",
    "class CustomerData(BaseModel):\n",
    "    # Define your expected features here:\n",
    "\n",
    "    Gender: str = None  # Make all fields optional, or provide default values\n",
    "    SeniorCitizen: str = Field(None, alias=\"Senior Citizen\")\n",
    "    Partner: str = None\n",
    "    Dependents: str = None\n",
    "    TenureMonths: float = Field(None, alias=\"Tenure Months\")\n",
    "    PhoneService: str = Field(None, alias=\"Phone Service\")\n",
    "    MultipleLines: str = Field(None, alias=\"Multiple Lines\")\n",
    "    InternetService: str = Field(None, alias=\"Internet Service\")\n",
    "    OnlineSecurity: str = Field(None, alias=\"Online Security\")\n",
    "    OnlineBackup: str = Field(None, alias=\"Online Backup\")\n",
    "    DeviceProtection: str = Field(None, alias=\"Device Protection\")\n",
    "    TechSupport: str = Field(None, alias=\"Tech Support\")\n",
    "    StreamingTV: str = Field(None, alias=\"Streaming TV\")\n",
    "    StreamingMovies: str = Field(None, alias=\"Streaming Movies\")\n",
    "    Contract: str = None\n",
    "    PaperlessBilling: str = Field(None, alias=\"Paperless Billing\")\n",
    "    PaymentMethod: str = Field(None, alias=\"Payment Method\")\n",
    "    MonthlyCharges: float = Field(None, alias=\"Monthly Charges\")\n",
    "    TotalCharges: str = Field(None, alias=\"Total Charges\")\n",
    "class Config:\n",
    "        allow_population_by_field_name = True\n",
    "        \n",
    "@app.post(\"/predict\")\n",
    "async def predict_endpoint(request: Request, data: Union[CustomerData, List[CustomerData], Dict, List[Dict]]): \n",
    "    print(data)  # Print the data\n",
    "    import logging\n",
    "    logging.info(f\"Received data: {data}\") # Log the data\n",
    "    # Accept single or batch\n",
    "    \"\"\"Endpoint for making churn predictions.\"\"\"\n",
    "\n",
    "    model_path = \"churn_model.pkl\"\n",
    "    scaler_path = \"churn_scaler.pkl\"\n",
    "\n",
    "    try:\n",
    "        if isinstance(data, CustomerData):  # Single prediction\n",
    "            data = data.dict()\n",
    "        elif isinstance(data, list) and all(isinstance(item, CustomerData) for item in data): # Batch prediction of validated data\n",
    "            data = [item.dict() for item in data]\n",
    "        elif isinstance(data, dict): # Single prediction with a dictionary\n",
    "            data = data\n",
    "        elif isinstance(data, list) and all(isinstance(item, dict) for item in data): # Batch prediction with list of dictionaries\n",
    "            data = data\n",
    "        else:\n",
    "            raise ValueError(\"Invalid data format. Please provide a dictionary or a list of dictionaries or Pydantic model instances.\")\n",
    "\n",
    "        predictions = predict_churn(data, model_path, scaler_path)\n",
    "        return predictions\n",
    "\n",
    "    except ValueError as ve:\n",
    "        raise HTTPException(status_code=400, detail=str(ve))  # Bad Request\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"An error occurred: {str(e)}\")  # Internal Server Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def extract_sentiment(text_column, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "    \"\"\"\n",
    "    Extracts sentiment from a text column and returns the updated DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        text_column (pd.Series): Column containing text data\n",
    "        model_name (str): Hugging Face model name for sentiment analysis\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Series containing sentiment labels\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    # Function to get sentiment for a single text\n",
    "    def get_sentiment(text):\n",
    "        # Handle NaN values gracefully\n",
    "        if pd.isna(text) or text.strip() == \"\":\n",
    "            return \"Neutral\"\n",
    "        \n",
    "        # Tokenize and get model outputs\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predicted label\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        return model.config.id2label[predicted_class_id]\n",
    "    \n",
    "    # Apply sentiment extraction to the text column\n",
    "    return text_column.apply(get_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Telco_customer_churn_with_text.csv\")\n",
    "df['customer_sentiment'] = extract_sentiment(df['customer_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Lat Long</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Gender</th>\n",
       "      <th>...</th>\n",
       "      <th>Monthly Charges</th>\n",
       "      <th>Total Charges</th>\n",
       "      <th>Churn Label</th>\n",
       "      <th>Churn Value</th>\n",
       "      <th>Churn Score</th>\n",
       "      <th>CLTV</th>\n",
       "      <th>Churn Reason</th>\n",
       "      <th>conversation</th>\n",
       "      <th>customer_text</th>\n",
       "      <th>customer_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90003</td>\n",
       "      <td>33.964131, -118.272783</td>\n",
       "      <td>33.964131</td>\n",
       "      <td>-118.272783</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>3239</td>\n",
       "      <td>Competitor made better offer</td>\n",
       "      <td>Customer: I have a question about my services....</td>\n",
       "      <td>I have a question about my services. I'm a new...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90005</td>\n",
       "      <td>34.059281, -118.30742</td>\n",
       "      <td>34.059281</td>\n",
       "      <td>-118.307420</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>2701</td>\n",
       "      <td>Moved</td>\n",
       "      <td>Customer: I have a question about my services....</td>\n",
       "      <td>I have a question about my services. I'm a new...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9305-CDSKC</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90006</td>\n",
       "      <td>34.048013, -118.293953</td>\n",
       "      <td>34.048013</td>\n",
       "      <td>-118.293953</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>99.65</td>\n",
       "      <td>820.5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>5372</td>\n",
       "      <td>Moved</td>\n",
       "      <td>Customer: I have a question about my services....</td>\n",
       "      <td>I have a question about my services. I'm a new...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7892-POOKP</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90010</td>\n",
       "      <td>34.062125, -118.315709</td>\n",
       "      <td>34.062125</td>\n",
       "      <td>-118.315709</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>104.80</td>\n",
       "      <td>3046.05</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>5003</td>\n",
       "      <td>Moved</td>\n",
       "      <td>Customer: I need help with my Fiber optic serv...</td>\n",
       "      <td>I need help with my Fiber optic service setup....</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0280-XJGEX</td>\n",
       "      <td>1</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90015</td>\n",
       "      <td>34.039224, -118.266293</td>\n",
       "      <td>34.039224</td>\n",
       "      <td>-118.266293</td>\n",
       "      <td>Male</td>\n",
       "      <td>...</td>\n",
       "      <td>103.70</td>\n",
       "      <td>5036.3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>5340</td>\n",
       "      <td>Competitor had better devices</td>\n",
       "      <td>Customer: I need help with my Fiber optic serv...</td>\n",
       "      <td>I need help with my Fiber optic service setup....</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Count        Country       State         City  Zip Code  \\\n",
       "0  3668-QPYBK      1  United States  California  Los Angeles     90003   \n",
       "1  9237-HQITU      1  United States  California  Los Angeles     90005   \n",
       "2  9305-CDSKC      1  United States  California  Los Angeles     90006   \n",
       "3  7892-POOKP      1  United States  California  Los Angeles     90010   \n",
       "4  0280-XJGEX      1  United States  California  Los Angeles     90015   \n",
       "\n",
       "                 Lat Long   Latitude   Longitude  Gender  ... Monthly Charges  \\\n",
       "0  33.964131, -118.272783  33.964131 -118.272783    Male  ...           53.85   \n",
       "1   34.059281, -118.30742  34.059281 -118.307420  Female  ...           70.70   \n",
       "2  34.048013, -118.293953  34.048013 -118.293953  Female  ...           99.65   \n",
       "3  34.062125, -118.315709  34.062125 -118.315709  Female  ...          104.80   \n",
       "4  34.039224, -118.266293  34.039224 -118.266293    Male  ...          103.70   \n",
       "\n",
       "  Total Charges Churn Label  Churn Value Churn Score  CLTV  \\\n",
       "0        108.15         Yes            1          86  3239   \n",
       "1        151.65         Yes            1          67  2701   \n",
       "2         820.5         Yes            1          86  5372   \n",
       "3       3046.05         Yes            1          84  5003   \n",
       "4        5036.3         Yes            1          89  5340   \n",
       "\n",
       "                    Churn Reason  \\\n",
       "0   Competitor made better offer   \n",
       "1                          Moved   \n",
       "2                          Moved   \n",
       "3                          Moved   \n",
       "4  Competitor had better devices   \n",
       "\n",
       "                                        conversation  \\\n",
       "0  Customer: I have a question about my services....   \n",
       "1  Customer: I have a question about my services....   \n",
       "2  Customer: I have a question about my services....   \n",
       "3  Customer: I need help with my Fiber optic serv...   \n",
       "4  Customer: I need help with my Fiber optic serv...   \n",
       "\n",
       "                                       customer_text customer_sentiment  \n",
       "0  I have a question about my services. I'm a new...           NEGATIVE  \n",
       "1  I have a question about my services. I'm a new...           NEGATIVE  \n",
       "2  I have a question about my services. I'm a new...           NEGATIVE  \n",
       "3  I need help with my Fiber optic service setup....           NEGATIVE  \n",
       "4  I need help with my Fiber optic service setup....           NEGATIVE  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def extract_and_reduce_features(text_column, model_name=\"bert-base-uncased\", n_components=10):\n",
    "    \"\"\"\n",
    "    Extracts text features using Hugging Face model, performs PCA to reduce dimensions,\n",
    "    and returns a DataFrame with the reduced features.\n",
    "    \n",
    "    Args:\n",
    "        text_column (pd.Series): Column containing text data\n",
    "        model_name (str): Hugging Face model name for feature extraction\n",
    "        n_components (int): Number of PCA components\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Original DataFrame with new feature columns added\n",
    "    \"\"\"\n",
    "    # Load feature extraction pipeline once\n",
    "    feature_extractor = pipeline(\n",
    "        \"feature-extraction\",\n",
    "        model=model_name,\n",
    "        framework=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Function to extract features for one text\n",
    "    def get_features(text):\n",
    "        # Handle NaN values gracefully\n",
    "        if pd.isna(text) or text.strip() == \"\":\n",
    "            return np.zeros((768,))  # Return zero vector for empty text\n",
    "        \n",
    "        # Extract features and take mean across tokens\n",
    "        features = feature_extractor(text, return_tensors=\"pt\")[0]\n",
    "        reduced_features = features.numpy().mean(axis=0)\n",
    "        return reduced_features\n",
    "    \n",
    "    # Apply feature extraction to the text column\n",
    "    feature_matrix = np.stack(text_column.apply(get_features))\n",
    "    \n",
    "    # Perform PCA to reduce the dimensionality to n_components\n",
    "    pca = PCA(n_components=1)\n",
    "    reduced_features = pca.fit_transform(feature_matrix)\n",
    "    \n",
    "    # Create DataFrame for the reduced features\n",
    "    feature_columns = [f'pca_feature_{i}' for i in range(n_components)]\n",
    "    feature_df = pd.DataFrame(reduced_features, columns=feature_columns)\n",
    "    \n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = {\n",
    "    'customer_text': [\n",
    "        \"My internet keeps disconnecting every 30 minutes. Very frustrating!\"\n",
    "        # \"The service is excellent, never had any issues with connectivity.\",\n",
    "        # \"WiFi signal is weak in my bedroom, need a solution urgently.\",\n",
    "        # \"Installation was quick and the internet speed is great.\",\n",
    "        # \"Cannot connect to the network during peak hours.\",\n",
    "        # \"Best internet service I've had in years. Streaming works perfectly.\",\n",
    "        # \"Router keeps restarting on its own. Please help!\",\n",
    "        # \"Download speeds are much slower than what I'm paying for.\",\n",
    "        # \"Great customer service, they fixed my connection issue quickly.\",\n",
    "        # \"Having trouble connecting multiple devices at once.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(customer_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_pca.py:591: RuntimeWarning: invalid value encountered in divide\n",
      "  explained_variance_ = (S**2) / (n_samples - 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 1), indices imply (1, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Extract features and reduce them to 10 components\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m reduced_features_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_and_reduce_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Concatenate the reduced features with the original DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, reduced_features_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 46\u001b[0m, in \u001b[0;36mextract_and_reduce_features\u001b[1;34m(text_column, model_name, n_components)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Create DataFrame for the reduced features\u001b[39;00m\n\u001b[0;32m     45\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca_feature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_components)]\n\u001b[1;32m---> 46\u001b[0m feature_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_df\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    825\u001b[0m         )\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (1, 1), indices imply (1, 10)"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract features and reduce them to 10 components\n",
    "reduced_features_df = extract_and_reduce_features(df['customer_text'])\n",
    "\n",
    "# Step 2: Concatenate the reduced features with the original DataFrame\n",
    "df = pd.concat([df, reduced_features_df], axis=1)\n",
    "\n",
    "# Step 3: Display the first few rows to check the result\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature DataFrame shape: (1, 10)\n",
      "\n",
      "Reduced features:\n",
      "   projected_feature_1  ...  projected_feature_10\n",
      "0             0.040793  ...             -0.734421\n",
      "\n",
      "[1 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"bert-base-uncased\", \n",
    "                 api_key: str = 'hf_qcFlKgxJRkwTpPggNQnwTLLPpXV',\n",
    "                 n_components: int = 10):\n",
    "        self.feature_extractor = pipeline(\n",
    "            \"feature-extraction\",\n",
    "            framework=\"pt\",\n",
    "            model=model_name,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        # Initialize random projection\n",
    "        self.random_projection = GaussianRandomProjection(n_components=n_components, random_state=42)\n",
    "        \n",
    "    def get_features(self, text: str, return_df: bool = True) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract features from text and reduce dimensions using Random Projection.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text string\n",
    "            return_df: If True, returns pandas DataFrame; if False, returns numpy array\n",
    "            \n",
    "        Returns:\n",
    "            Reduced features as either numpy array or pandas DataFrame\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(text, return_tensors=\"pt\")[0]\n",
    "        reduced_features = features.numpy().mean(axis=0)\n",
    "        \n",
    "        # Reshape for random projection (needs 2D array)\n",
    "        features_2d = reduced_features.reshape(1, -1)\n",
    "        \n",
    "        # Apply random projection\n",
    "        projected_features = self.random_projection.fit_transform(features_2d)\n",
    "        \n",
    "        if return_df:\n",
    "            feature_columns = [f'projected_feature_{i+1}' for i in range(projected_features.shape[1])]\n",
    "            return pd.DataFrame(projected_features, columns=feature_columns)\n",
    "        \n",
    "        return projected_features\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize extractor\n",
    "    extractor = TextFeatureExtractor(n_components=10)\n",
    "    \n",
    "    # Single text example\n",
    "    text = \"My internet keeps going down. What's the problem!?\"\n",
    "    \n",
    "    # Get reduced features\n",
    "    features_df = extractor.get_features(text)\n",
    "    print(\"\\nFeature DataFrame shape:\", features_df.shape)\n",
    "    print(\"\\nReduced features:\")\n",
    "    print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"Gender\": \"Female\",\n",
    "        \"Senior Citizen\": \"No\",\n",
    "        \"Partner\": \"Yes\",\n",
    "        \"Dependents\": \"No\",\n",
    "        \"Tenure Months\": 24,\n",
    "        \"Phone Service\": \"Yes\",\n",
    "        \"Multiple Lines\": \"No\",\n",
    "        \"Internet Service\": \"DSL\",\n",
    "        \"Online Security\": \"Yes\",\n",
    "        \"Online Backup\": \"No\",\n",
    "        \"Device Protection\": \"Yes\",\n",
    "        \"Tech Support\": \"No\",\n",
    "        \"Streaming TV\": \"Yes\",\n",
    "        \"Streaming Movies\": \"No\",\n",
    "        \"Contract\": \"Month-to-month\",\n",
    "        \"Paperless Billing\": \"Yes\",\n",
    "        \"Payment Method\": \"Electronic check\",\n",
    "        \"Monthly Charges\": 65.6,\n",
    "        \"Total Charges\": 1576.45,\n",
    "  \"customer_text\": \"My internet is always slow. What's happening with your service?\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:data_preparation3:Successfully loaded dataset with shape: (1, 20)\n",
      "INFO:data_preparation3:Data validation completed\n",
      "Device set to use cpu\n",
      "INFO:data_preparation3:Data preparation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data_preparation3 import DataPreparation\n",
    "prep = DataPreparation()\n",
    "new_df = pd.DataFrame([data], index=[0]) # Add an index!\n",
    "new_df, validation_report = prep.prepare_data(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Senior Citizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Tenure Months</th>\n",
       "      <th>Phone Service</th>\n",
       "      <th>Multiple Lines</th>\n",
       "      <th>Online Security</th>\n",
       "      <th>Online Backup</th>\n",
       "      <th>Device Protection</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>pca_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243202</td>\n",
       "      <td>-0.283105</td>\n",
       "      <td>-2.904098</td>\n",
       "      <td>4.149575</td>\n",
       "      <td>-5.649469</td>\n",
       "      <td>-1.911078</td>\n",
       "      <td>-0.257134</td>\n",
       "      <td>-1.217405</td>\n",
       "      <td>1.038977</td>\n",
       "      <td>-0.025525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender Senior Citizen Partner Dependents  Tenure Months Phone Service  \\\n",
       "0  Female             No     Yes         No            0.0           Yes   \n",
       "\n",
       "  Multiple Lines Online Security Online Backup Device Protection  ...  \\\n",
       "0             No             Yes            No               Yes  ...   \n",
       "\n",
       "      pca_0     pca_1     pca_2     pca_3     pca_4     pca_5     pca_6  \\\n",
       "0 -0.243202 -0.283105 -2.904098  4.149575 -5.649469 -1.911078 -0.257134   \n",
       "\n",
       "      pca_7     pca_8     pca_9  \n",
       "0 -1.217405  1.038977 -0.025525  \n",
       "\n",
       "[1 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_preparation:Successfully loaded dataset with shape: (7043, 35)\n",
      "INFO:data_preparation:Data validation completed\n",
      "Device set to use cpu\n",
      "INFO:data_preparation:Data preparation completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Run the entire data preparation process\n",
    "from data_preparation import DataPreparation\n",
    "prep = DataPreparation()\n",
    "\n",
    "df = pd.read_csv(\"./data/Telco_customer_churn_with_text.csv\")\n",
    "new_df, validation_report = prep.prepare_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
